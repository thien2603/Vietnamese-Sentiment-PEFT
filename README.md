# Vietnamese-Sentiment-PEFT
# NghiÃªn cá»©u So sÃ¡nh Full Fine-Tuning vÃ  LoRA (PEFT) cho PhoBERT

ÄÃ¢y lÃ  project thá»±c hiá»‡n so sÃ¡nh hiá»‡u suáº¥t vÃ  hiá»‡u quáº£ cá»§a hai phÆ°Æ¡ng phÃ¡p fine-tuning mÃ´ hÃ¬nh `vinai/phobert-base` trÃªn bÃ i toÃ¡n PhÃ¢n loáº¡i Cáº£m xÃºc Tiáº¿ng Viá»‡t (Dataset: `uitnlp/vietnamese_students_feedback`).

## 1. Má»¥c tiÃªu Project

NghiÃªn cá»©u nÃ y tÃ¬m cÃ¡ch tráº£ lá»i cÃ¡c cÃ¢u há»i:
* Ká»¹ thuáº­t LoRA (PEFT) tiáº¿t kiá»‡m Ä‘Æ°á»£c bao nhiÃªu % tÃ i nguyÃªn (tham sá»‘ huáº¥n luyá»‡n) so vá»›i Full Fine-Tuning?
* Viá»‡c tiáº¿t kiá»‡m tÃ i nguyÃªn Ä‘Ã³ cÃ³ pháº£i Ä‘Ã¡nh Ä‘á»•i báº±ng hiá»‡u suáº¥t (F1-score) hay khÃ´ng?

## 2. Káº¿t quáº£ ThÃ­ nghiá»‡m ğŸ“Š

Káº¿t quáº£ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn Google Colab (GPU T4) vá»›i 3 Epochs, `batch_size=32`.

| PhÆ°Æ¡ng phÃ¡p | Test F1-macro | Thá»i gian train (giÃ¢y) | Sá»‘ tham sá»‘ train | Tá»‰ lá»‡ tham sá»‘ |
|:---|---:|---:|:---|---:|
| 1. Full Fine-Tuning (Baseline) | **0.8208** | **305.62** | 135,000,579 | 100.00000% |
| 2. LoRA (PEFT) (`r=8`) | 0.6126 | 402.71* | **1,035,267** | **0.76686%** |

*(Báº£ng nÃ y Ä‘Æ°á»£c copy 100% tá»« output BÆ¯á»šC 5 cá»§a báº¡n)*

---

## 3. PhÃ¢n tÃ­ch & Nháº­n xÃ©t

Tá»« báº£ng káº¿t quáº£ trÃªn, chÃºng ta cÃ³ thá»ƒ rÃºt ra 3 nháº­n xÃ©t quan trá»ng:

### âœ… 1. Hiá»‡u quáº£ Tham sá»‘ (Parameter Efficiency)
LoRA Ä‘Ã£ chá»©ng minh hiá»‡u quáº£ vÆ°á»£t trá»™i trong viá»‡c tiáº¿t kiá»‡m tÃ i nguyÃªn, chá»‰ cáº§n huáº¥n luyá»‡n **1,035,267** tham sá»‘. Con sá»‘ nÃ y chá»‰ báº±ng **0.77%** so vá»›i 135 triá»‡u tham sá»‘ cá»§a phÆ°Æ¡ng phÃ¡p Full Fine-Tuning.

### âš ï¸ 2. Hiá»‡u suáº¥t MÃ´ hÃ¬nh (Model Performance)
Vá»›i cÃ i Ä‘áº·t máº·c Ä‘á»‹nh (`r=8`), hiá»‡u suáº¥t cá»§a LoRA **giáº£m sÃºt Ä‘Ã¡ng ká»ƒ**. MÃ´ hÃ¬nh LoRA chá»‰ Ä‘áº¡t **0.6126 F1-macro**, tÆ°Æ¡ng Ä‘Æ°Æ¡ng **74.63%** hiá»‡u suáº¥t so vá»›i baseline (0.8208). Äiá»u nÃ y cho tháº¥y LoRA khÃ´ng pháº£i lÃºc nÃ o cÅ©ng lÃ  giáº£i phÃ¡p thay tháº¿ trá»±c tiáº¿p mÃ  khÃ´ng cáº§n tinh chá»‰nh.

### (*) 3. Ghi chÃº vá» Thá»i gian Huáº¥n luyá»‡n
Thá»i gian huáº¥n luyá»‡n cá»§a LoRA (402.71s) trong thÃ­ nghiá»‡m nÃ y cao hÆ¡n Baseline (305.62s). ÄÃ¢y lÃ  má»™t káº¿t quáº£ **báº¥t thÆ°á»ng**, gÃ¢y ra bá»Ÿi viá»‡c script huáº¥n luyá»‡n LoRA Ä‘Ã£ kÃ­ch hoáº¡t vÃ  **chá» ngÆ°á»i dÃ¹ng nháº­p API key cá»§a Weights & Biases (W&B)**.

Do Ä‘Ã³, phÃ©p so sÃ¡nh vá» tá»‘c Ä‘á»™ trong láº§n cháº¡y nÃ y lÃ  **khÃ´ng há»£p lá»‡**.

## 4. Káº¿t luáº­n & HÆ°á»›ng phÃ¡t triá»ƒn
* **Káº¿t luáº­n:** LoRA lÃ  má»™t ká»¹ thuáº­t tuyá»‡t vá»i Ä‘á»ƒ tiáº¿t kiá»‡m tÃ i nguyÃªn, nhÆ°ng viá»‡c Ã¡p dá»¥ng thÃ nh cÃ´ng Ä‘Ã²i há»i pháº£i tinh chá»‰nh siÃªu tham sá»‘ (hyperparameter) cáº©n tháº­n.
* **HÆ°á»›ng phÃ¡t triá»ƒn (Future Work):** Káº¿t quáº£ F1-score tháº¥p cá»§a LoRA (0.61) má»Ÿ ra cÃ¡c hÆ°á»›ng thÃ­ nghiá»‡m tiáº¿p theo Ä‘á»ƒ cáº£i thiá»‡n:
    1.  TÄƒng rank cá»§a LoRA (vÃ­ dá»¥: `r=16` hoáº·c `r=32`).
    2.  Tinh chá»‰nh `learning_rate` riÃªng biá»‡t cho LoRA.
    3.  TÄƒng sá»‘ `epochs` huáº¥n luyá»‡n (vÃ¬ LoRA há»™i tá»¥ cháº­m hÆ¡n).

## 5. CÃ¡ch cháº¡y láº¡i (Reproducibility)
1.  Má»Ÿ file `[TÃªn file .ipynb cá»§a báº¡n]` trong Google Colab.
2.  Chá»n `Runtime` -> `Change runtime type` -> `T4 GPU`.
3.  Cháº¡y táº¥t cáº£ cÃ¡c cell. (LÆ°u Ã½: Äá»ƒ so sÃ¡nh thá»i gian chÃ­nh xÃ¡c, hÃ£y thÃªm `report_to="none"` vÃ o `TrainingArguments` cá»§a LoRA Ä‘á»ƒ táº¯t W&B).

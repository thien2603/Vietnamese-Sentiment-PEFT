# Ph√¢n t√≠ch So s√°nh Full Fine-Tuning v√† LoRA (PEFT) cho PhoBERT

ƒê√¢y l√† project nghi√™n c·ª©u so s√°nh hi·ªáu su·∫•t v√† hi·ªáu qu·∫£ c·ªßa b·ªën ph∆∞∆°ng ph√°p fine-tuning m√¥ h√¨nh `vinai/phobert-base` tr√™n b√†i to√°n Ph√¢n lo·∫°i C·∫£m x√∫c Ti·∫øng Vi·ªát (b·ªô d·ªØ li·ªáu 3 nh√£n: NEG, NEU, POS).

## 1. M·ª•c ti√™u Project

Nghi√™n c·ª©u n√†y th·ª±c hi·ªán m·ªôt chu·ªói th√≠ nghi·ªám c√≥ h·ªá th·ªëng ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi:
* **Hi·ªáu qu·∫£ (Efficiency):** K·ªπ thu·∫≠t LoRA (PEFT) gi√∫p ti·∫øt ki·ªám t√†i nguy√™n (s·ªë tham s·ªë, th·ªùi gian hu·∫•n luy·ªán) ƒë·∫øn m·ª©c n√†o so v·ªõi Full Fine-Tuning?
* **Hi·ªáu su·∫•t (Performance):** Ch√∫ng ta c√≥ ph·∫£i hy sinh ƒë·ªô ch√≠nh x√°c (F1-score) ƒë·ªÉ ƒë·ªïi l·∫•y hi·ªáu qu·∫£ kh√¥ng?
* **ƒêi·ªÉm t·ªëi ∆∞u (Sweet Spot):** Rank (r) c·ªßa LoRA b·∫±ng bao nhi√™u (`r=8, 16, hay 32`) l√† t·ªëi ∆∞u nh·∫•t cho b√†i to√°n n√†y?

## 2. K·∫øt qu·∫£ So s√°nh üìä

ƒê√¢y l√† b·∫£ng k·∫øt qu·∫£ cu·ªëi c√πng, ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ c√°c l·∫ßn ch·∫°y th√≠ nghi·ªám.

| Ph∆∞∆°ng ph√°p | Test F1-macro | Th·ªùi gian train (s) | S·ªë tham s·ªë train | T·ªâ l·ªá tham s·ªë |
|:---|---:|---:|:---|---:|
| 1. Full Fine-Tuning (Baseline) | 0.7124 | 479.52 | 135,000,579 | 100.00% |
| 2. LoRA (PEFT) r=8 | 0.6571 | **70.50** | 1,035,267 | **0.77%** |
| 3. LoRA (PEFT) r=256 | **0.7127** | 104.58 | 14,748,675 | 10.92% |
| 4. LoRA (PEFT) r=512 | 0.7064 | 133.33 | 28,904,451 | 21.41% |

*(L∆∞u √Ω: B·∫°n n√™n c·∫≠p nh·∫≠t b·∫£ng n√†y v·ªõi k·∫øt qu·∫£ cu·ªëi c√πng ch√≠nh x√°c nh·∫•t c·ªßa m√¨nh)*

## 3. Ph√¢n t√≠ch & K·∫øt lu·∫≠n

* **LoRA r=8:** Nhanh nh·∫•t nh∆∞ng hi·ªáu su·∫•t th·∫•p, b·ªã underfit.
* **LoRA r=16 (ƒêi·ªÉm ng·ªçt üèÜ):** ƒê·∫°t F1-score t∆∞∆°ng ƒë∆∞∆°ng Baseline, nhanh h∆°n 4.6 l·∫ßn, ch·ªâ d√πng 10.9% tham s·ªë.
* **LoRA r=32:** Hi·ªáu su·∫•t gi·∫£m nh·∫π, t·ªën t√†i nguy√™n h∆°n r=16.

**K·∫øt lu·∫≠n:** Th√≠ nghi·ªám ch·ª©ng minh **LoRA (r=16)** l√† chi·∫øn l∆∞·ª£c t·ªëi ∆∞u, c√¢n b·∫±ng ho√†n h·∫£o gi·ªØa hi·ªáu su·∫•t v√† hi·ªáu qu·∫£ cho b√†i to√°n n√†y.

---
## 4. C√°ch ch·∫°y l·∫°i (How to Run) üöÄ

C√≥ 2 c√°ch ƒë·ªÉ ch·∫°y l·∫°i c√°c th√≠ nghi·ªám trong project n√†y:

### C√°ch 1: Ch·∫°y b·∫±ng Script `.py` (Chuy√™n nghi·ªáp)

C√°ch n√†y ph√π h·ª£p n·∫øu b·∫°n mu·ªën ch·∫°y th√≠ nghi·ªám tr√™n m√°y local ho·∫∑c server c√≥ GPU.

**B∆∞·ªõc 1: Chu·∫©n b·ªã M√¥i tr∆∞·ªùng**

1.  Clone repository n√†y v·ªÅ m√°y c·ªßa b·∫°n:
    ```bash
    git clone [URL-repo-GitHub-c·ªßa-b·∫°n]
    cd [T√™n-repo-c·ªßa-b·∫°n]
    ```
2.  (Khuy·∫øn ngh·ªã) T·∫°o v√† k√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o (virtual environment):
    ```bash
    python -m venv venv
    # Windows:
    .\venv\Scripts\activate
    # macOS/Linux:
    source venv/bin/activate
    ```
3.  C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:
    ```bash
    pip install -r requirements.txt
    ```

**B∆∞·ªõc 2: Ch·∫°y Th√≠ nghi·ªám**

S·ª≠ d·ª•ng script `train.py` v·ªõi c√°c tham s·ªë d√≤ng l·ªánh:

* **Ch·∫°y Baseline (Full Fine-Tuning):**
    ```bash
    python train.py --output_dir "results/baseline"
    ```
* **Ch·∫°y LoRA v·ªõi Rank=16:**
    ```bash
    python train.py --output_dir "results/lora_r16" --use_lora --lora_rank 16
    ```
* **Ch·∫°y LoRA v·ªõi Rank=32:**
    ```bash
    python train.py --output_dir "results/lora_r32" --use_lora --lora_rank 32
    ```
* **(T√πy ch·ªçn)** Ch·∫°y v·ªõi c√°c tham s·ªë kh√°c (v√≠ d·ª•: 5 epochs, batch size 16):
    ```bash
    python train.py --output_dir "results/lora_r16_5epochs" --use_lora --lora_rank 16 --num_epochs 5 --batch_size 16
    ```

**L∆∞u √Ω:** Script s·∫Ω t·ª± ƒë·ªông t·∫£i dataset t·ª´ Hugging Face. ƒê·∫£m b·∫£o b·∫°n c√≥ k·∫øt n·ªëi Internet ·ªïn ƒë·ªãnh.

---
### C√°ch 2: Ch·∫°y b·∫±ng Notebook `.ipynb` tr√™n Google Colab (Tr·ª±c quan) üß™

C√°ch n√†y ph√π h·ª£p ƒë·ªÉ xem l·∫°i to√†n b·ªô qu√° tr√¨nh th√≠ nghi·ªám, output chi ti·∫øt v√† ch·∫°y nhanh tr√™n GPU mi·ªÖn ph√≠ c·ªßa Google.

**B∆∞·ªõc 1: M·ªü Google Colab**

1.  Truy c·∫≠p [colab.research.google.com](https://colab.research.google.com/).
2.  Ch·ªçn **`File`** -> **`Upload notebook...`** (T·∫£i s·ªï tay l√™n...).
3.  Ch·ªçn tab **`GitHub`**.
4.  D√°n **URL c·ªßa repo GitHub** n√†y v√†o √¥ t√¨m ki·∫øm v√† nh·∫•n Enter.
5.  Ch·ªçn file notebook (v√≠ d·ª•: `PhoBERT_PEFT_Analysis.ipynb`) t·ª´ danh s√°ch.

**B∆∞·ªõc 2: Ch·ªçn Runtime GPU**

1.  Sau khi notebook m·ªü ra, ch·ªçn **`Runtime`** (Th·ªùi gian ch·∫°y) tr√™n thanh menu.
2.  Ch·ªçn **`Change runtime type`** (Thay ƒë·ªïi lo·∫°i th·ªùi gian ch·∫°y).
3.  Trong m·ª•c "Hardware accelerator" (Tr√¨nh tƒÉng t·ªëc ph·∫ßn c·ª©ng), ch·ªçn **`T4 GPU`** (ho·∫∑c GPU kh√°c n·∫øu c√≥).
4.  Nh·∫•n **`Save`** (L∆∞u).

**B∆∞·ªõc 3: Ch·∫°y Notebook**

1.  Ch·ªçn **`Runtime`** (Th·ªùi gian ch·∫°y) tr√™n thanh menu.
2.  Ch·ªçn **`Run all`** (Ch·∫°y t·∫•t c·∫£).

Notebook s·∫Ω t·ª± ƒë·ªông c√†i ƒë·∫∑t th∆∞ vi·ªán, t·∫£i data, ch·∫°y l·∫ßn l∆∞·ª£t 4 th√≠ nghi·ªám (Baseline, LoRA r=8, r=16, r=32) v√† in ra b·∫£ng so s√°nh k·∫øt qu·∫£ cu·ªëi c√πng. Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t kho·∫£ng 20-30 ph√∫t.
